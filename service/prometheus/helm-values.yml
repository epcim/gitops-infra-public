# vim: ft=yaml syntax=yaml sts=2 ts=2 sw=2

#defaultRules:
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml#L33

fullnameOverride: prometheus-stack

prometheusOperator:
  createCustomResource: true
  # prometheusConfigReloaderImage:
  #   repository: quay.io/coreos/prometheus-config-reloader
  #   tag: v0.39.0
  # configmapReloadImage:
  #   repository: jimmidyson/configmap-reload
  #   tag: v0.4.0
  prometheusConfigReloader:
    resources:
      limits:
        cpu: 200m
  admissionWebhooks:
    enabled: false
  tls:               # required for disabled webhooks
    enabled: false   # OK for non-prod deployments

alertmanager:
  enabled: true
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          resources:
            requests:
              storage: 10Gi
    tolerations:
    - key: "arm"
      operator: "Exists"
    #podMetadata:
    #  annotations:
    #    backup.velero.io/backup-volumes: alertmanager-kube-prometheus-stack-alertmanager-db
    podAntiAffinity: "hard"

  ingress:
    enabled: true
    pathType: Prefix
    annotations:
       kubernetes.io/ingress.class: "traefik"
    #  nginx.ingress.kubernetes.io/auth-url: "https://auth.apealive.net/oauth2/auth"
    #  nginx.ingress.kubernetes.io/auth-signin: https://auth.apealive.net/oauth2/start
       traefik.ingress.kubernetes.io/router.entrypoints: websecure
       traefik.ingress.kubernetes.io/router.tls: "true"
       hajimari.io/enable: "false"
       traefik.frontend.rule.type: PathPrefix

    hosts:
    - prom-alert.apealive.net
    # FIXME, kustomize breaks build if this exists (file path too long, not exist, etc..)
    #####  paths:
    #####  - path: /
    #####    pathType: Prefix
    tls:
    - hosts:
      - prom-alert.apealive.net

  # https://prometheus.io/docs/alerting/configuration/#configuration-file
  # https://prometheus.io/webtools/alerting/routing-tree-editor/
  config:
    global:
      resolve_timeout: 5m
  ### CONFIG SECTIION ###
      http_config:

    receivers:
    - name: discord
      webhook_configs:
      - url: '{{ secrets.alertmanager.discord.webhook_url }}'
    # - name: 'telegram'
    #   telegram_configs:
    #     api_url:
    #     chat_id:
    - name: blackhole-receiver


  ### CONFIG SECTIION ###
    route:
      group_by: ['alertname', 'job']
      ## group_wait: 30s
      ## group_interval: 5m
      ## repeat_interval: 6h
      receiver: blackhole-receiver
      routes:
      - matchers:
        - severity =~ "warning|critical"
        receiver: discord
        continue: true
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      # Apply inhibition if the alertname is the same.
      equal: ['alertname', 'namespace']
    templates: ["*.tmpl"]

###
nodeExporter:
  enabled: true

prometheus-node-exporter:
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: instance
  #tolerations:
  #- key: "arm"
  #  operator: "Exists"
  #- key: "armhf"
  #  operator: "Exists"
  #- key: "node-role.kubernetes.io/master"
  #  operator: "Exists"

###
grafana:
  enabled: false
  forceDeployDashboards: true

###
kubelet:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      - action: replace
        sourceLabels:
          - node
        targetLabel: instance

kubeApiServer:
  enabled: true

kubeControllerManager:
  enabled: true
  endpoints:
    - {{ secrets.vip_kube }}
  service:
    enabled: true
    port: 10257
    targetPort: 10257
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true

kubeScheduler:
  enabled: true
  endpoints:
    - {{ secrets.vip_kube }}
  service:
    enabled: true
    port: 10259
    targetPort: 10259
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true

kubeProxy:
  enabled: true
  endpoints:
    - {{ secrets.vip_kube }}

kubeEtcd:
  enabled: true
  endpoints:
    - {{ secrets.vip_kube }}
  service:
    enabled: true
    port: 12379 #2381
    targetPort: 12379 # 2381


###
prometheus:
  ingress:
    enabled: true
    pathType: Prefix
    annotations:
      kubernetes.io/ingress.class: "traefik"
      #nginx.ingress.kubernetes.io/auth-url: "https://auth.apealive.net/oauth2/auth"
      #nginx.ingress.kubernetes.io/auth-signin: https://auth.apealive.net/oauth2/start
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      traefik.ingress.kubernetes.io/router.tls: "true"
      hajimari.io/enable: "false"
      traefik.frontend.rule.type: PathPrefix
    hosts:
    - prom-server.apealive.net
    tls:
    - hosts:
      - prom-server.apealive.net

  prometheusSpec:
    # image:
    #   repository: quay.io/prometheus/prometheus
    #   tag: v2.20.0
    replicas: 1 # FIXME
    replicaExternalLabelName: "replica"
    ruleSelector: {}
    ruleNamespaceSelector: {}
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelector: {}
    podMonitorNamespaceSelector: {}
    podMonitorSelectorNilUsesHelmValues: false
    retention: 6h
    enableAdminAPI: true
    walCompression: true
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          resources:
            requests:
              storage: 10Gi
    ## tolerations:
    ## - key: "arm"
    ##   operator: "Exists"
    #podMetadata:
    #  annotations:
    #    backup.velero.io/backup-volumes: prometheus-kube-prometheus-stack-prometheus-db
    ### CONFIG SECTION ###
    additionalScrapeConfigs:
    #- job_name: 'node'
    #  static_configs:
    #  - targets:
    #    - FIXME_IP_OR_FQDN:9100
    - job_name: 'home-assistant'
      scrape_interval: 60s
      metrics_path: '/api/prometheus'
      bearer_token: '{{ secrets.prometheus.hass_token }}'
      scheme: http
      static_configs:
      - targets:
        - hass-home-assistant.home.svc:8123
    # - job_name: 'upsc-exporter'
    #   metrics_path: '/metrics'
    #   static_configs:
    #   - targets:
    #     - pi4-c.home:8081
    ### CONFIG SECTION ###

    thanos:
      image: quay.io/thanos/thanos:v0.30.2
      version: v0.30.2
      objectStorageConfig:
        name: thanos-objstore-secret
        key: objstore.yml
    resources:
      requests:
        memory: 2000Mi
        cpu: 250m
      limits:
        memory: 4000Mi
    # affinity:
    #   podAntiAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #     - labelSelector:
    #         matchExpressions:
    #         - key: app.kubernetes.io/name
    #           operator: In
    #           values:
    #           - prometheus
    #       topologyKey: "kubernetes.io/hostname"
    podAntiAffinity: "hard"
  thanosService:
    enabled: true
  thanosServiceMonitor:
    enabled: true

additionalAlertRelabelConfigs:
